{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d0b2e1-4047-41de-8e8b-6e778412854e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.340211Z",
     "iopub.status.busy": "2025-03-02T15:45:43.339212Z",
     "iopub.status.idle": "2025-03-02T15:45:43.360680Z",
     "shell.execute_reply": "2025-03-02T15:45:43.358662Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.340211Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import  DataSource\n",
    "import Utils\n",
    "\n",
    "zz500 = DataSource.get_zz500_codes() # 我看中证500的数据\n",
    "import talib\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619410a-717b-46a8-8da4-a16a87f38a53",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d33019-92c4-4c20-b5e5-86757faaad31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.365679Z",
     "iopub.status.busy": "2025-03-02T15:45:43.364681Z",
     "iopub.status.idle": "2025-03-02T15:45:43.375659Z",
     "shell.execute_reply": "2025-03-02T15:45:43.373641Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.365679Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "batch_size = 64 # 一个批次有多少个数据\n",
    "column_names = [\n",
    "    'macd', 'macd signal', 'macd hist',\n",
    "                'slowk', 'slowd', 'slowj',\n",
    "                'rsi6', 'rsi12', 'rsi24',\n",
    "                'bias_6', 'bias_12', 'bias_24',\n",
    "                'willr'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c9ea9-f2cd-4dcd-83f0-bb067791c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb57a5b-a6d5-4325-a00d-033d8d5858cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.378658Z",
     "iopub.status.busy": "2025-03-02T15:45:43.377660Z",
     "iopub.status.idle": "2025-03-02T15:45:43.410394Z",
     "shell.execute_reply": "2025-03-02T15:45:43.406377Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.378658Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loader(code_name, batch_size=16, train_rate=0.9):\n",
    "    dt = DataSource.get_data(code_name)\n",
    "    # 添加几个指标\n",
    "    macd, signal, hist = talib.MACD(dt['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    dt['macd'] = macd\n",
    "    dt['macd signal'] = signal\n",
    "    dt['macd hist'] = hist\n",
    "    # \n",
    "    # 初始化KDJ参数，使用EMA计算\n",
    "    fastk_period = 9\n",
    "    slowk_period = 3\n",
    "    slowd_period = 3\n",
    "    \n",
    "    # 调用STOCH函数计算KDJ，注意ma_type=1表示使用EMA\n",
    "    slowk, slowd = talib.STOCH(\n",
    "        dt['high'],\n",
    "        dt['low'],\n",
    "        dt['close'],\n",
    "        fastk_period,\n",
    "        2 * slowk_period - 1, # 转换为EMA所需的周期\n",
    "        1, # ma_type设置为1表示使用EMA\n",
    "        2 * slowd_period - 1, # 转换为EMA所需的周期\n",
    "        1 # ma_type设置为1表示使用EMA\n",
    "        )\n",
    "    dt['slowk'] = slowk\n",
    "    dt['slowd'] = slowd\n",
    "    # 计算J值\n",
    "    dt['slowj'] = 3 * slowk - 2 * slowd\n",
    "    # rsi\n",
    "    dt[\"rsi6\"] = talib.RSI(dt['close'], timeperiod=6)\n",
    "    dt[\"rsi12\"] = talib.RSI(dt['close'], timeperiod=12)\n",
    "    dt[\"rsi24\"] = talib.RSI(dt['close'], timeperiod=24)\n",
    "    #\n",
    "    dt['bias_6'] = (dt['close'] - dt['close'].rolling(6, min_periods=1).mean())/ dt['close'].rolling(6, min_periods=1).mean()\n",
    "    dt['bias_12'] = (dt['close'] - dt['close'].rolling(12, min_periods=1).mean())/ dt['close'].rolling(12, min_periods=1).mean()\n",
    "    dt['bias_24'] = (dt['close'] - dt['close'].rolling(24, min_periods=1).mean())/ dt['close'].rolling(24, min_periods=1).mean()\n",
    "    # \n",
    "    dt['willr'] = talib.WILLR(dt['high'], dt['low'], dt['close'], timeperiod=14)\n",
    "    # 计算涨幅\n",
    "    dt['rate'] = (dt['close'].shift(-1)-dt['close'])/dt['close']\n",
    "    # 然后删除含有nan值的行\n",
    "    dt2 = dt.dropna(axis=0, how='any')\n",
    "    # 然后做成整数倍\n",
    "    x_data = dt2.loc[:, column_names].to_numpy()\n",
    "    y_data = dt2.loc[:, ['rate']].to_numpy()\n",
    "    _x_y_len = int(x_data.shape[0]/batch_size) * batch_size\n",
    "    x_price = x_data[-_x_y_len:]\n",
    "    y_close = y_data[-_x_y_len:]\n",
    "    # 这里需要转换一下维度\n",
    "    x_price = x_price.reshape((_x_y_len//batch_size, batch_size, len(column_names)))\n",
    "    y_close = y_close.reshape(_x_y_len//batch_size, batch_size, 1)\n",
    "    # 做成加载器\n",
    "    dataset = Data.TensorDataset(\n",
    "        torch.FloatTensor(x_price),\n",
    "        torch.FloatTensor(y_close))\n",
    "    train_loader, test_loader = Data.random_split(\n",
    "        dataset,\n",
    "        lengths=[\n",
    "            int(train_rate*len(dataset)),\n",
    "            len(dataset)-int(train_rate*len(dataset))],\n",
    "        generator=torch.Generator().manual_seed(0))\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0515bb1-5207-40d2-ba83-b9ec3be52d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.416413Z",
     "iopub.status.busy": "2025-03-02T15:45:43.414398Z",
     "iopub.status.idle": "2025-03-02T15:45:43.587400Z",
     "shell.execute_reply": "2025-03-02T15:45:43.585397Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.416413Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loader(\n",
    "    zz500[0],\n",
    "    batch_size=batch_size,\n",
    "    train_rate=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63cc4e-49cf-4975-8324-66118ae57913",
   "metadata": {},
   "source": [
    "# 做网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fe62641-85a6-436f-9c76-d9dda495ccca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.592400Z",
     "iopub.status.busy": "2025-03-02T15:45:43.591403Z",
     "iopub.status.idle": "2025-03-02T15:45:43.642638Z",
     "shell.execute_reply": "2025-03-02T15:45:43.637637Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.592400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一个网络\n",
    "class MACD_RSI_KDJ_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MACD_RSI_KDJ_1, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(column_names), 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # 全连接层\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8030a3e0-4b18-464b-abf0-c134b69cc375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.652637Z",
     "iopub.status.busy": "2025-03-02T15:45:43.651660Z",
     "iopub.status.idle": "2025-03-02T15:45:43.823421Z",
     "shell.execute_reply": "2025-03-02T15:45:43.821426Z",
     "shell.execute_reply.started": "2025-03-02T15:45:43.652637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (linear0): Linear(in_features=13, out_features=128, bias=True)\n",
       "  (relu0): LeakyReLU(negative_slope=0.01)\n",
       "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (linear3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu4): LeakyReLU(negative_slope=0.01)\n",
       "  (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu5): LeakyReLU(negative_slope=0.01)\n",
       "  (linear6): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu6): LeakyReLU(negative_slope=0.01)\n",
       "  (linear7): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu7): LeakyReLU(negative_slope=0.01)\n",
       "  (linear8): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全连接层的特征数\n",
    "linear_features = [len(column_names), 128, 128, 128, 128, 128, 128, 128,128 ]\n",
    "arr_nets = []\n",
    "for i in range(len(linear_features)-1):\n",
    "    arr_nets.append((\n",
    "        f'linear{i}',\n",
    "        nn.Linear(linear_features[i], linear_features[i+1])))\n",
    "    arr_nets.append((f'relu{i}', nn.LeakyReLU()))\n",
    "# 最后一个全连接层\n",
    "arr_nets.append((f'linear{len(linear_features)-1}', nn.Linear(linear_features[-1], 1)))\n",
    "net = torch.nn.Sequential(OrderedDict(arr_nets)).to(device)\n",
    "net\n",
    "# net = torch.nn.Sequential(\n",
    "#     nn.Linear(len(column_names), 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(256, 512),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 1),\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73517589-d9df-44ab-828e-865d51e4e75e",
   "metadata": {},
   "source": [
    "# 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e1086-a17a-4487-82ad-0ed6e033e2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:45:43.830424Z",
     "iopub.status.busy": "2025-03-02T15:45:43.825424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, avg labels: 0.01415 avg loss:0.01614, max loss: 0.04649, min loss : 0.00727\n",
      "2, avg labels: 0.01415 avg loss:0.01451, max loss: 0.04063, min loss : 0.00672\n",
      "3, avg labels: 0.01415 avg loss:0.01439, max loss: 0.04063, min loss : 0.00677\n",
      "4, avg labels: 0.01415 avg loss:0.01449, max loss: 0.04066, min loss : 0.00671\n",
      "5, avg labels: 0.01415 avg loss:0.01434, max loss: 0.04087, min loss : 0.00656\n",
      "6, avg labels: 0.01415 avg loss:0.01427, max loss: 0.04084, min loss : 0.00655\n",
      "7, avg labels: 0.01415 avg loss:0.01429, max loss: 0.04076, min loss : 0.00650\n",
      "8, avg labels: 0.01415 avg loss:0.01429, max loss: 0.04073, min loss : 0.00652\n",
      "9, avg labels: 0.01415 avg loss:0.01427, max loss: 0.04081, min loss : 0.00654\n",
      "10, avg labels: 0.01415 avg loss:0.01425, max loss: 0.04083, min loss : 0.00651\n",
      "11, avg labels: 0.01415 avg loss:0.01424, max loss: 0.04091, min loss : 0.00651\n",
      "12, avg labels: 0.01415 avg loss:0.01428, max loss: 0.04115, min loss : 0.00657\n",
      "13, avg labels: 0.01415 avg loss:0.01421, max loss: 0.04084, min loss : 0.00651\n",
      "14, avg labels: 0.01415 avg loss:0.01421, max loss: 0.04077, min loss : 0.00649\n",
      "15, avg labels: 0.01415 avg loss:0.01426, max loss: 0.04115, min loss : 0.00676\n",
      "16, avg labels: 0.01415 avg loss:0.01428, max loss: 0.04119, min loss : 0.00673\n",
      "17, avg labels: 0.01415 avg loss:0.01421, max loss: 0.04108, min loss : 0.00673\n",
      "18, avg labels: 0.01415 avg loss:0.01422, max loss: 0.04126, min loss : 0.00685\n",
      "19, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04092, min loss : 0.00670\n",
      "20, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04105, min loss : 0.00681\n",
      "21, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04093, min loss : 0.00658\n",
      "22, avg labels: 0.01415 avg loss:0.01425, max loss: 0.04130, min loss : 0.00681\n",
      "23, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04099, min loss : 0.00679\n",
      "24, avg labels: 0.01415 avg loss:0.01422, max loss: 0.04122, min loss : 0.00693\n",
      "25, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04087, min loss : 0.00672\n",
      "26, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04115, min loss : 0.00687\n",
      "27, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04118, min loss : 0.00696\n",
      "28, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04090, min loss : 0.00684\n",
      "29, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04111, min loss : 0.00691\n",
      "30, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04102, min loss : 0.00682\n",
      "31, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04100, min loss : 0.00690\n",
      "32, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04105, min loss : 0.00686\n",
      "33, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04119, min loss : 0.00691\n",
      "34, avg labels: 0.01415 avg loss:0.01414, max loss: 0.04097, min loss : 0.00669\n",
      "35, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04123, min loss : 0.00682\n",
      "36, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04115, min loss : 0.00684\n",
      "37, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04124, min loss : 0.00686\n",
      "38, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04126, min loss : 0.00699\n",
      "39, avg labels: 0.01415 avg loss:0.01421, max loss: 0.04137, min loss : 0.00694\n",
      "40, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04131, min loss : 0.00693\n",
      "41, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04136, min loss : 0.00697\n",
      "42, avg labels: 0.01415 avg loss:0.01422, max loss: 0.04141, min loss : 0.00705\n",
      "43, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04137, min loss : 0.00697\n",
      "44, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04144, min loss : 0.00686\n",
      "45, avg labels: 0.01415 avg loss:0.01423, max loss: 0.04157, min loss : 0.00689\n",
      "46, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04154, min loss : 0.00690\n",
      "47, avg labels: 0.01415 avg loss:0.01424, max loss: 0.04161, min loss : 0.00691\n",
      "48, avg labels: 0.01415 avg loss:0.01419, max loss: 0.04147, min loss : 0.00684\n",
      "49, avg labels: 0.01415 avg loss:0.01419, max loss: 0.04157, min loss : 0.00696\n",
      "50, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04163, min loss : 0.00682\n",
      "51, avg labels: 0.01415 avg loss:0.01419, max loss: 0.04167, min loss : 0.00675\n",
      "52, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04138, min loss : 0.00689\n",
      "53, avg labels: 0.01415 avg loss:0.01411, max loss: 0.04136, min loss : 0.00678\n",
      "54, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04153, min loss : 0.00667\n",
      "55, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04159, min loss : 0.00675\n",
      "56, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04162, min loss : 0.00677\n",
      "57, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04173, min loss : 0.00679\n",
      "58, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04147, min loss : 0.00671\n",
      "59, avg labels: 0.01415 avg loss:0.01419, max loss: 0.04174, min loss : 0.00672\n",
      "60, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04145, min loss : 0.00642\n",
      "61, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04115, min loss : 0.00667\n",
      "62, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04147, min loss : 0.00659\n",
      "63, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04167, min loss : 0.00685\n",
      "64, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04164, min loss : 0.00676\n",
      "65, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04174, min loss : 0.00658\n",
      "66, avg labels: 0.01415 avg loss:0.01423, max loss: 0.04184, min loss : 0.00674\n",
      "67, avg labels: 0.01415 avg loss:0.01427, max loss: 0.04186, min loss : 0.00687\n",
      "68, avg labels: 0.01415 avg loss:0.01422, max loss: 0.04184, min loss : 0.00685\n",
      "69, avg labels: 0.01415 avg loss:0.01420, max loss: 0.04176, min loss : 0.00681\n",
      "70, avg labels: 0.01415 avg loss:0.01418, max loss: 0.04172, min loss : 0.00643\n",
      "71, avg labels: 0.01415 avg loss:0.01417, max loss: 0.04155, min loss : 0.00673\n",
      "72, avg labels: 0.01415 avg loss:0.01413, max loss: 0.04167, min loss : 0.00631\n",
      "73, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04168, min loss : 0.00679\n",
      "74, avg labels: 0.01415 avg loss:0.01412, max loss: 0.04159, min loss : 0.00664\n",
      "75, avg labels: 0.01415 avg loss:0.01412, max loss: 0.04166, min loss : 0.00665\n",
      "76, avg labels: 0.01415 avg loss:0.01414, max loss: 0.04156, min loss : 0.00672\n",
      "77, avg labels: 0.01415 avg loss:0.01412, max loss: 0.04164, min loss : 0.00652\n",
      "78, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04163, min loss : 0.00686\n",
      "79, avg labels: 0.01415 avg loss:0.01410, max loss: 0.04144, min loss : 0.00646\n",
      "80, avg labels: 0.01415 avg loss:0.01412, max loss: 0.04168, min loss : 0.00659\n",
      "81, avg labels: 0.01415 avg loss:0.01410, max loss: 0.04169, min loss : 0.00656\n",
      "82, avg labels: 0.01415 avg loss:0.01416, max loss: 0.04170, min loss : 0.00646\n",
      "83, avg labels: 0.01415 avg loss:0.01413, max loss: 0.04172, min loss : 0.00654\n",
      "84, avg labels: 0.01415 avg loss:0.01410, max loss: 0.04151, min loss : 0.00636\n",
      "85, avg labels: 0.01415 avg loss:0.01407, max loss: 0.04164, min loss : 0.00655\n",
      "86, avg labels: 0.01415 avg loss:0.01414, max loss: 0.04167, min loss : 0.00664\n",
      "87, avg labels: 0.01415 avg loss:0.01411, max loss: 0.04161, min loss : 0.00623\n",
      "88, avg labels: 0.01415 avg loss:0.01409, max loss: 0.04149, min loss : 0.00627\n",
      "89, avg labels: 0.01415 avg loss:0.01414, max loss: 0.04164, min loss : 0.00656\n",
      "90, avg labels: 0.01415 avg loss:0.01410, max loss: 0.04143, min loss : 0.00664\n",
      "91, avg labels: 0.01415 avg loss:0.01412, max loss: 0.04149, min loss : 0.00684\n",
      "92, avg labels: 0.01415 avg loss:0.01409, max loss: 0.04144, min loss : 0.00641\n",
      "93, avg labels: 0.01415 avg loss:0.01407, max loss: 0.04152, min loss : 0.00644\n",
      "94, avg labels: 0.01415 avg loss:0.01404, max loss: 0.04127, min loss : 0.00637\n",
      "95, avg labels: 0.01415 avg loss:0.01403, max loss: 0.04139, min loss : 0.00649\n",
      "96, avg labels: 0.01415 avg loss:0.01403, max loss: 0.04131, min loss : 0.00637\n",
      "97, avg labels: 0.01415 avg loss:0.01409, max loss: 0.04154, min loss : 0.00650\n",
      "98, avg labels: 0.01415 avg loss:0.01409, max loss: 0.04150, min loss : 0.00638\n",
      "99, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04171, min loss : 0.00630\n",
      "100, avg labels: 0.01415 avg loss:0.01407, max loss: 0.04130, min loss : 0.00647\n",
      "101, avg labels: 0.01415 avg loss:0.01413, max loss: 0.04156, min loss : 0.00664\n",
      "102, avg labels: 0.01415 avg loss:0.01406, max loss: 0.04130, min loss : 0.00653\n",
      "103, avg labels: 0.01415 avg loss:0.01408, max loss: 0.04152, min loss : 0.00643\n",
      "104, avg labels: 0.01415 avg loss:0.01411, max loss: 0.04162, min loss : 0.00641\n",
      "105, avg labels: 0.01415 avg loss:0.01405, max loss: 0.04155, min loss : 0.00643\n",
      "106, avg labels: 0.01415 avg loss:0.01401, max loss: 0.04118, min loss : 0.00628\n",
      "107, avg labels: 0.01415 avg loss:0.01407, max loss: 0.04149, min loss : 0.00643\n",
      "108, avg labels: 0.01415 avg loss:0.01405, max loss: 0.04166, min loss : 0.00621\n",
      "109, avg labels: 0.01415 avg loss:0.01406, max loss: 0.04141, min loss : 0.00623\n",
      "110, avg labels: 0.01415 avg loss:0.01415, max loss: 0.04160, min loss : 0.00646\n",
      "111, avg labels: 0.01415 avg loss:0.01402, max loss: 0.04135, min loss : 0.00633\n",
      "112, avg labels: 0.01415 avg loss:0.01407, max loss: 0.04162, min loss : 0.00660\n",
      "113, avg labels: 0.01415 avg loss:0.01405, max loss: 0.04150, min loss : 0.00600\n",
      "114, avg labels: 0.01415 avg loss:0.01399, max loss: 0.04114, min loss : 0.00649\n",
      "115, avg labels: 0.01415 avg loss:0.01397, max loss: 0.04120, min loss : 0.00644\n",
      "116, avg labels: 0.01415 avg loss:0.01401, max loss: 0.04124, min loss : 0.00653\n",
      "117, avg labels: 0.01415 avg loss:0.01401, max loss: 0.04139, min loss : 0.00619\n",
      "118, avg labels: 0.01415 avg loss:0.01402, max loss: 0.04155, min loss : 0.00631\n",
      "119, avg labels: 0.01415 avg loss:0.01395, max loss: 0.04123, min loss : 0.00668\n",
      "120, avg labels: 0.01415 avg loss:0.01408, max loss: 0.04131, min loss : 0.00630\n",
      "121, avg labels: 0.01415 avg loss:0.01393, max loss: 0.04124, min loss : 0.00630\n",
      "122, avg labels: 0.01415 avg loss:0.01390, max loss: 0.04106, min loss : 0.00614\n",
      "123, avg labels: 0.01415 avg loss:0.01398, max loss: 0.04143, min loss : 0.00638\n",
      "124, avg labels: 0.01415 avg loss:0.01393, max loss: 0.04098, min loss : 0.00641\n",
      "125, avg labels: 0.01415 avg loss:0.01387, max loss: 0.04114, min loss : 0.00624\n",
      "126, avg labels: 0.01415 avg loss:0.01400, max loss: 0.04121, min loss : 0.00645\n",
      "127, avg labels: 0.01415 avg loss:0.01399, max loss: 0.04116, min loss : 0.00655\n",
      "128, avg labels: 0.01415 avg loss:0.01387, max loss: 0.04113, min loss : 0.00620\n",
      "129, avg labels: 0.01415 avg loss:0.01385, max loss: 0.04116, min loss : 0.00661\n",
      "130, avg labels: 0.01415 avg loss:0.01388, max loss: 0.04118, min loss : 0.00645\n",
      "131, avg labels: 0.01415 avg loss:0.01393, max loss: 0.04123, min loss : 0.00650\n",
      "132, avg labels: 0.01415 avg loss:0.01380, max loss: 0.04086, min loss : 0.00647\n",
      "133, avg labels: 0.01415 avg loss:0.01389, max loss: 0.04095, min loss : 0.00647\n",
      "134, avg labels: 0.01415 avg loss:0.01390, max loss: 0.04138, min loss : 0.00625\n",
      "135, avg labels: 0.01415 avg loss:0.01399, max loss: 0.04140, min loss : 0.00640\n",
      "136, avg labels: 0.01415 avg loss:0.01384, max loss: 0.04100, min loss : 0.00655\n",
      "137, avg labels: 0.01415 avg loss:0.01381, max loss: 0.04111, min loss : 0.00638\n",
      "138, avg labels: 0.01415 avg loss:0.01380, max loss: 0.04089, min loss : 0.00658\n",
      "139, avg labels: 0.01415 avg loss:0.01377, max loss: 0.04081, min loss : 0.00625\n",
      "140, avg labels: 0.01415 avg loss:0.01380, max loss: 0.04108, min loss : 0.00632\n",
      "141, avg labels: 0.01415 avg loss:0.01378, max loss: 0.04099, min loss : 0.00632\n",
      "142, avg labels: 0.01415 avg loss:0.01387, max loss: 0.04088, min loss : 0.00634\n",
      "143, avg labels: 0.01415 avg loss:0.01381, max loss: 0.04113, min loss : 0.00608\n",
      "144, avg labels: 0.01415 avg loss:0.01389, max loss: 0.04118, min loss : 0.00617\n",
      "145, avg labels: 0.01415 avg loss:0.01376, max loss: 0.04084, min loss : 0.00649\n",
      "146, avg labels: 0.01415 avg loss:0.01376, max loss: 0.04102, min loss : 0.00614\n",
      "147, avg labels: 0.01415 avg loss:0.01373, max loss: 0.04091, min loss : 0.00631\n",
      "148, avg labels: 0.01415 avg loss:0.01367, max loss: 0.04102, min loss : 0.00630\n",
      "149, avg labels: 0.01415 avg loss:0.01369, max loss: 0.04082, min loss : 0.00622\n",
      "150, avg labels: 0.01415 avg loss:0.01380, max loss: 0.04101, min loss : 0.00618\n",
      "151, avg labels: 0.01415 avg loss:0.01374, max loss: 0.04118, min loss : 0.00636\n",
      "152, avg labels: 0.01415 avg loss:0.01380, max loss: 0.04122, min loss : 0.00632\n",
      "153, avg labels: 0.01415 avg loss:0.01370, max loss: 0.04107, min loss : 0.00636\n",
      "154, avg labels: 0.01415 avg loss:0.01367, max loss: 0.04075, min loss : 0.00627\n",
      "155, avg labels: 0.01415 avg loss:0.01365, max loss: 0.04083, min loss : 0.00642\n",
      "156, avg labels: 0.01415 avg loss:0.01371, max loss: 0.04094, min loss : 0.00608\n",
      "157, avg labels: 0.01415 avg loss:0.01362, max loss: 0.04062, min loss : 0.00628\n",
      "158, avg labels: 0.01415 avg loss:0.01349, max loss: 0.04065, min loss : 0.00633\n",
      "159, avg labels: 0.01415 avg loss:0.01358, max loss: 0.04037, min loss : 0.00601\n",
      "160, avg labels: 0.01415 avg loss:0.01363, max loss: 0.04061, min loss : 0.00610\n",
      "161, avg labels: 0.01415 avg loss:0.01369, max loss: 0.04072, min loss : 0.00632\n",
      "162, avg labels: 0.01415 avg loss:0.01359, max loss: 0.04040, min loss : 0.00591\n",
      "163, avg labels: 0.01415 avg loss:0.01357, max loss: 0.04027, min loss : 0.00630\n",
      "164, avg labels: 0.01415 avg loss:0.01350, max loss: 0.04043, min loss : 0.00616\n",
      "165, avg labels: 0.01415 avg loss:0.01360, max loss: 0.04082, min loss : 0.00608\n",
      "166, avg labels: 0.01415 avg loss:0.01364, max loss: 0.04098, min loss : 0.00623\n",
      "167, avg labels: 0.01415 avg loss:0.01357, max loss: 0.04076, min loss : 0.00652\n",
      "168, avg labels: 0.01415 avg loss:0.01362, max loss: 0.04032, min loss : 0.00666\n",
      "169, avg labels: 0.01415 avg loss:0.01345, max loss: 0.04033, min loss : 0.00603\n",
      "170, avg labels: 0.01415 avg loss:0.01343, max loss: 0.04035, min loss : 0.00616\n",
      "171, avg labels: 0.01415 avg loss:0.01339, max loss: 0.04000, min loss : 0.00578\n",
      "172, avg labels: 0.01415 avg loss:0.01346, max loss: 0.04082, min loss : 0.00599\n",
      "173, avg labels: 0.01415 avg loss:0.01345, max loss: 0.04061, min loss : 0.00583\n",
      "174, avg labels: 0.01415 avg loss:0.01353, max loss: 0.04071, min loss : 0.00591\n",
      "175, avg labels: 0.01415 avg loss:0.01351, max loss: 0.04079, min loss : 0.00575\n",
      "176, avg labels: 0.01415 avg loss:0.01350, max loss: 0.04051, min loss : 0.00601\n",
      "177, avg labels: 0.01415 avg loss:0.01346, max loss: 0.04046, min loss : 0.00576\n",
      "178, avg labels: 0.01415 avg loss:0.01336, max loss: 0.04056, min loss : 0.00593\n",
      "179, avg labels: 0.01415 avg loss:0.01340, max loss: 0.04080, min loss : 0.00619\n",
      "180, avg labels: 0.01415 avg loss:0.01359, max loss: 0.04059, min loss : 0.00611\n",
      "181, avg labels: 0.01415 avg loss:0.01360, max loss: 0.04042, min loss : 0.00586\n",
      "182, avg labels: 0.01415 avg loss:0.01358, max loss: 0.04054, min loss : 0.00619\n",
      "183, avg labels: 0.01415 avg loss:0.01359, max loss: 0.04047, min loss : 0.00606\n",
      "184, avg labels: 0.01415 avg loss:0.01340, max loss: 0.04026, min loss : 0.00597\n",
      "185, avg labels: 0.01415 avg loss:0.01338, max loss: 0.04006, min loss : 0.00604\n",
      "186, avg labels: 0.01415 avg loss:0.01328, max loss: 0.04005, min loss : 0.00579\n",
      "187, avg labels: 0.01415 avg loss:0.01322, max loss: 0.03958, min loss : 0.00610\n",
      "188, avg labels: 0.01415 avg loss:0.01318, max loss: 0.03937, min loss : 0.00624\n",
      "189, avg labels: 0.01415 avg loss:0.01309, max loss: 0.03925, min loss : 0.00589\n",
      "190, avg labels: 0.01415 avg loss:0.01306, max loss: 0.03948, min loss : 0.00568\n",
      "191, avg labels: 0.01415 avg loss:0.01320, max loss: 0.03986, min loss : 0.00572\n",
      "192, avg labels: 0.01415 avg loss:0.01317, max loss: 0.03980, min loss : 0.00554\n",
      "193, avg labels: 0.01415 avg loss:0.01330, max loss: 0.04014, min loss : 0.00557\n",
      "194, avg labels: 0.01415 avg loss:0.01312, max loss: 0.03959, min loss : 0.00557\n",
      "195, avg labels: 0.01415 avg loss:0.01287, max loss: 0.03943, min loss : 0.00535\n",
      "196, avg labels: 0.01415 avg loss:0.01283, max loss: 0.03940, min loss : 0.00556\n",
      "197, avg labels: 0.01415 avg loss:0.01279, max loss: 0.03912, min loss : 0.00544\n",
      "198, avg labels: 0.01415 avg loss:0.01270, max loss: 0.03900, min loss : 0.00526\n",
      "199, avg labels: 0.01415 avg loss:0.01266, max loss: 0.03874, min loss : 0.00561\n",
      "200, avg labels: 0.01415 avg loss:0.01270, max loss: 0.03900, min loss : 0.00575\n",
      "201, avg labels: 0.01415 avg loss:0.01264, max loss: 0.03894, min loss : 0.00560\n",
      "202, avg labels: 0.01415 avg loss:0.01255, max loss: 0.03933, min loss : 0.00524\n",
      "203, avg labels: 0.01415 avg loss:0.01263, max loss: 0.03946, min loss : 0.00526\n",
      "204, avg labels: 0.01415 avg loss:0.01261, max loss: 0.03946, min loss : 0.00584\n",
      "205, avg labels: 0.01415 avg loss:0.01273, max loss: 0.03904, min loss : 0.00583\n",
      "206, avg labels: 0.01415 avg loss:0.01262, max loss: 0.03943, min loss : 0.00567\n",
      "207, avg labels: 0.01415 avg loss:0.01261, max loss: 0.04002, min loss : 0.00576\n",
      "208, avg labels: 0.01415 avg loss:0.01249, max loss: 0.03957, min loss : 0.00524\n",
      "209, avg labels: 0.01415 avg loss:0.01235, max loss: 0.03920, min loss : 0.00532\n",
      "210, avg labels: 0.01415 avg loss:0.01231, max loss: 0.03935, min loss : 0.00507\n",
      "211, avg labels: 0.01415 avg loss:0.01226, max loss: 0.03919, min loss : 0.00549\n",
      "212, avg labels: 0.01415 avg loss:0.01223, max loss: 0.03891, min loss : 0.00518\n",
      "213, avg labels: 0.01415 avg loss:0.01225, max loss: 0.03932, min loss : 0.00526\n",
      "214, avg labels: 0.01415 avg loss:0.01227, max loss: 0.03918, min loss : 0.00522\n",
      "215, avg labels: 0.01415 avg loss:0.01224, max loss: 0.03969, min loss : 0.00558\n",
      "216, avg labels: 0.01415 avg loss:0.01221, max loss: 0.03995, min loss : 0.00581\n",
      "217, avg labels: 0.01415 avg loss:0.01246, max loss: 0.04022, min loss : 0.00619\n",
      "218, avg labels: 0.01415 avg loss:0.01239, max loss: 0.03941, min loss : 0.00560\n",
      "219, avg labels: 0.01415 avg loss:0.01212, max loss: 0.03904, min loss : 0.00577\n",
      "220, avg labels: 0.01415 avg loss:0.01210, max loss: 0.03930, min loss : 0.00522\n",
      "221, avg labels: 0.01415 avg loss:0.01200, max loss: 0.03878, min loss : 0.00542\n",
      "222, avg labels: 0.01415 avg loss:0.01181, max loss: 0.03849, min loss : 0.00532\n",
      "223, avg labels: 0.01415 avg loss:0.01178, max loss: 0.03841, min loss : 0.00545\n",
      "224, avg labels: 0.01415 avg loss:0.01160, max loss: 0.03782, min loss : 0.00535\n",
      "225, avg labels: 0.01415 avg loss:0.01171, max loss: 0.03850, min loss : 0.00562\n",
      "226, avg labels: 0.01415 avg loss:0.01165, max loss: 0.03787, min loss : 0.00539\n",
      "227, avg labels: 0.01415 avg loss:0.01160, max loss: 0.03743, min loss : 0.00536\n",
      "228, avg labels: 0.01415 avg loss:0.01148, max loss: 0.03743, min loss : 0.00548\n",
      "229, avg labels: 0.01415 avg loss:0.01168, max loss: 0.03754, min loss : 0.00570\n",
      "230, avg labels: 0.01415 avg loss:0.01176, max loss: 0.03782, min loss : 0.00567\n",
      "231, avg labels: 0.01415 avg loss:0.01155, max loss: 0.03759, min loss : 0.00572\n",
      "232, avg labels: 0.01415 avg loss:0.01146, max loss: 0.03725, min loss : 0.00573\n",
      "233, avg labels: 0.01415 avg loss:0.01149, max loss: 0.03731, min loss : 0.00572\n",
      "234, avg labels: 0.01415 avg loss:0.01133, max loss: 0.03772, min loss : 0.00560\n",
      "235, avg labels: 0.01415 avg loss:0.01157, max loss: 0.03716, min loss : 0.00548\n",
      "236, avg labels: 0.01415 avg loss:0.01177, max loss: 0.03891, min loss : 0.00557\n",
      "237, avg labels: 0.01415 avg loss:0.01156, max loss: 0.03748, min loss : 0.00544\n",
      "238, avg labels: 0.01415 avg loss:0.01148, max loss: 0.03805, min loss : 0.00581\n",
      "239, avg labels: 0.01415 avg loss:0.01156, max loss: 0.03708, min loss : 0.00564\n",
      "240, avg labels: 0.01415 avg loss:0.01139, max loss: 0.03668, min loss : 0.00527\n",
      "241, avg labels: 0.01415 avg loss:0.01149, max loss: 0.03657, min loss : 0.00556\n",
      "242, avg labels: 0.01415 avg loss:0.01130, max loss: 0.03652, min loss : 0.00493\n",
      "243, avg labels: 0.01415 avg loss:0.01151, max loss: 0.03705, min loss : 0.00539\n",
      "244, avg labels: 0.01415 avg loss:0.01147, max loss: 0.03641, min loss : 0.00570\n",
      "245, avg labels: 0.01415 avg loss:0.01174, max loss: 0.03689, min loss : 0.00578\n",
      "246, avg labels: 0.01415 avg loss:0.01173, max loss: 0.03713, min loss : 0.00539\n",
      "247, avg labels: 0.01415 avg loss:0.01155, max loss: 0.03695, min loss : 0.00585\n",
      "248, avg labels: 0.01415 avg loss:0.01158, max loss: 0.03606, min loss : 0.00585\n",
      "249, avg labels: 0.01415 avg loss:0.01160, max loss: 0.03683, min loss : 0.00571\n",
      "250, avg labels: 0.01415 avg loss:0.01149, max loss: 0.03661, min loss : 0.00636\n",
      "251, avg labels: 0.01415 avg loss:0.01197, max loss: 0.03761, min loss : 0.00586\n",
      "252, avg labels: 0.01415 avg loss:0.01222, max loss: 0.03734, min loss : 0.00593\n",
      "253, avg labels: 0.01415 avg loss:0.01215, max loss: 0.03932, min loss : 0.00523\n",
      "254, avg labels: 0.01415 avg loss:0.01164, max loss: 0.03807, min loss : 0.00506\n",
      "255, avg labels: 0.01415 avg loss:0.01148, max loss: 0.03806, min loss : 0.00454\n",
      "256, avg labels: 0.01415 avg loss:0.01128, max loss: 0.03674, min loss : 0.00484\n",
      "257, avg labels: 0.01415 avg loss:0.01115, max loss: 0.03646, min loss : 0.00513\n",
      "258, avg labels: 0.01415 avg loss:0.01117, max loss: 0.03716, min loss : 0.00514\n",
      "259, avg labels: 0.01415 avg loss:0.01106, max loss: 0.03722, min loss : 0.00477\n",
      "260, avg labels: 0.01415 avg loss:0.01110, max loss: 0.03656, min loss : 0.00508\n",
      "261, avg labels: 0.01415 avg loss:0.01097, max loss: 0.03660, min loss : 0.00507\n",
      "262, avg labels: 0.01415 avg loss:0.01088, max loss: 0.03634, min loss : 0.00481\n",
      "263, avg labels: 0.01415 avg loss:0.01098, max loss: 0.03633, min loss : 0.00526\n",
      "264, avg labels: 0.01415 avg loss:0.01104, max loss: 0.03689, min loss : 0.00523\n",
      "265, avg labels: 0.01415 avg loss:0.01082, max loss: 0.03620, min loss : 0.00449\n",
      "266, avg labels: 0.01415 avg loss:0.01093, max loss: 0.03678, min loss : 0.00456\n",
      "267, avg labels: 0.01415 avg loss:0.01088, max loss: 0.03612, min loss : 0.00485\n",
      "268, avg labels: 0.01415 avg loss:0.01078, max loss: 0.03675, min loss : 0.00506\n",
      "269, avg labels: 0.01415 avg loss:0.01073, max loss: 0.03627, min loss : 0.00469\n",
      "270, avg labels: 0.01415 avg loss:0.01060, max loss: 0.03648, min loss : 0.00471\n",
      "271, avg labels: 0.01415 avg loss:0.01073, max loss: 0.03665, min loss : 0.00496\n",
      "272, avg labels: 0.01415 avg loss:0.01078, max loss: 0.03587, min loss : 0.00485\n",
      "273, avg labels: 0.01415 avg loss:0.01093, max loss: 0.03768, min loss : 0.00500\n",
      "274, avg labels: 0.01415 avg loss:0.01095, max loss: 0.03625, min loss : 0.00508\n",
      "275, avg labels: 0.01415 avg loss:0.01105, max loss: 0.03595, min loss : 0.00492\n",
      "276, avg labels: 0.01415 avg loss:0.01105, max loss: 0.03643, min loss : 0.00526\n",
      "277, avg labels: 0.01415 avg loss:0.01086, max loss: 0.03517, min loss : 0.00508\n",
      "278, avg labels: 0.01415 avg loss:0.01077, max loss: 0.03517, min loss : 0.00533\n",
      "279, avg labels: 0.01415 avg loss:0.01081, max loss: 0.03567, min loss : 0.00557\n",
      "280, avg labels: 0.01415 avg loss:0.01082, max loss: 0.03478, min loss : 0.00546\n",
      "281, avg labels: 0.01415 avg loss:0.01079, max loss: 0.03396, min loss : 0.00564\n",
      "282, avg labels: 0.01415 avg loss:0.01092, max loss: 0.03558, min loss : 0.00578\n",
      "283, avg labels: 0.01415 avg loss:0.01084, max loss: 0.03428, min loss : 0.00543\n",
      "284, avg labels: 0.01415 avg loss:0.01068, max loss: 0.03435, min loss : 0.00501\n",
      "285, avg labels: 0.01415 avg loss:0.01084, max loss: 0.03473, min loss : 0.00514\n",
      "286, avg labels: 0.01415 avg loss:0.01038, max loss: 0.03400, min loss : 0.00480\n",
      "287, avg labels: 0.01415 avg loss:0.00993, max loss: 0.03303, min loss : 0.00456\n",
      "288, avg labels: 0.01415 avg loss:0.00969, max loss: 0.03332, min loss : 0.00443\n",
      "289, avg labels: 0.01415 avg loss:0.00957, max loss: 0.03285, min loss : 0.00456\n",
      "290, avg labels: 0.01415 avg loss:0.00933, max loss: 0.03296, min loss : 0.00387\n",
      "291, avg labels: 0.01415 avg loss:0.00920, max loss: 0.03229, min loss : 0.00412\n",
      "292, avg labels: 0.01415 avg loss:0.00913, max loss: 0.03247, min loss : 0.00426\n",
      "293, avg labels: 0.01415 avg loss:0.00908, max loss: 0.03209, min loss : 0.00402\n",
      "294, avg labels: 0.01415 avg loss:0.00899, max loss: 0.03182, min loss : 0.00415\n",
      "295, avg labels: 0.01415 avg loss:0.00895, max loss: 0.03222, min loss : 0.00444\n",
      "296, avg labels: 0.01415 avg loss:0.00890, max loss: 0.03143, min loss : 0.00398\n",
      "297, avg labels: 0.01415 avg loss:0.00870, max loss: 0.03110, min loss : 0.00384\n",
      "298, avg labels: 0.01415 avg loss:0.00873, max loss: 0.03085, min loss : 0.00422\n",
      "299, avg labels: 0.01415 avg loss:0.00874, max loss: 0.03165, min loss : 0.00400\n",
      "300, avg labels: 0.01415 avg loss:0.00876, max loss: 0.03072, min loss : 0.00411\n",
      "301, avg labels: 0.01415 avg loss:0.00872, max loss: 0.03045, min loss : 0.00409\n",
      "302, avg labels: 0.01415 avg loss:0.00891, max loss: 0.03121, min loss : 0.00438\n",
      "303, avg labels: 0.01415 avg loss:0.00900, max loss: 0.03130, min loss : 0.00426\n",
      "304, avg labels: 0.01415 avg loss:0.00906, max loss: 0.03120, min loss : 0.00428\n",
      "305, avg labels: 0.01415 avg loss:0.00883, max loss: 0.03100, min loss : 0.00367\n",
      "306, avg labels: 0.01415 avg loss:0.00863, max loss: 0.03007, min loss : 0.00417\n",
      "307, avg labels: 0.01415 avg loss:0.00855, max loss: 0.03000, min loss : 0.00424\n",
      "308, avg labels: 0.01415 avg loss:0.00834, max loss: 0.03069, min loss : 0.00354\n",
      "309, avg labels: 0.01415 avg loss:0.00830, max loss: 0.03010, min loss : 0.00375\n",
      "310, avg labels: 0.01415 avg loss:0.00838, max loss: 0.02988, min loss : 0.00354\n",
      "311, avg labels: 0.01415 avg loss:0.00834, max loss: 0.03058, min loss : 0.00382\n",
      "312, avg labels: 0.01415 avg loss:0.00817, max loss: 0.02995, min loss : 0.00393\n",
      "313, avg labels: 0.01415 avg loss:0.00826, max loss: 0.03058, min loss : 0.00393\n",
      "314, avg labels: 0.01415 avg loss:0.00818, max loss: 0.02946, min loss : 0.00404\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.L1Loss()  # 绝对值误差函数\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "Utils.do_train(100, train_loader, net, optimizer, criterion)\n",
    "# 接下来做测试\n",
    "Utils.do_test(test_loader, net, criterion)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27047a7c-1f2e-4280-8f77-214679ca7702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9dc60-7c8b-453b-823e-7ec7dec85920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
